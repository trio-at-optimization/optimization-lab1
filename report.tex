\documentclass[12pt, a4paper, oneside]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{soulutf8}
\usepackage{soul}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage[most]{tcolorbox}
\usepackage{listings}
\usepackage[makeroom]{cancel}
\usepackage{tocloft}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{skak}
\usepackage{stmaryrd}
\usepackage{listings}
\usepackage{color}
\usepackage[framemethod = tikz]{mdframed}
\usepackage{svg}

\hypersetup{
	colorlinks,
	citecolor = black,
	filecolor = black,
	linkcolor = blue,
	urlcolor = blue
}
\lstset{
	basicstyle = \ttfamily,
	mathescape
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{\nouppercase{\rightmark\hfill\leftmark}}
\fancyhead[RO]{\nouppercase{\leftmark\hfill\rightmark}}
\fancyfoot[LE,RO]{\hfill\thepage\hfill}

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}

\definecolor{light-gray}{gray}{0.95}
\lstset{columns = fullflexible, basicstyle = \ttfamily, mathescape}
\surroundwithmdframed[hidealllines = true, backgroundcolor = light-gray, innerleftmargin = 15pt, innertopmargin = 0pt, innerbottommargin = 0pt]{lstlisting}

\everymath{\displaystyle}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №1}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ Казанков Владислав Константинович
		\end{tabular}
	}
	\vspace*{30em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Задача 1}
	\subsection*{Постановка задачи}
	Реализуйте градиентный спуск с постоянным шагом (learning rate).
	\subsection*{Решение}
	Поймем, изначальное, чего мы хотим добиться: мы хотели бы найти приближенный минимум на плоскости у заданной непрерывной функции $f$. Однако, при наивном решении этой задачи возникает проблема с производительностью нахождения $\operatorname{argmin}{f}$ за счет появления тех или иных накладных расходов на подсчет не целочисленных значения, а также всецело такого алгоритма, который бы с точность до некоторого изменения $\varepsilon$ не <<застрянет>> в бесконечном поиске интересующей точки.

	Для таких целей самым простым, но действующим метод является \textit{градиентный спуск с постоянным шагом}. 	Введем обозначения, пусть $\lambda$~-- есть некоторая константа, порядка $10^{-3}$, $x_{i} = \{x^{0}_{i}, ~ x^{1}_{i}, ~ \ldots, ~ x^{n - 1}_{i}\}$~-- некоторая координата в $n$-мерном пространстве, $p_{i}$~-- наше текущее направление.

	Теперь рассмотрим идею: оптимизацию нахождения необходимого минимум за $k$ шагов мы будем осуществлять шаги в $n$-мерном пространстве в направлении, задаваемый как антиградиент функции $f$ в точке, задаваемая предыдущем шагом, то есть
	\[
		x_{i + 1} = x_{i} - \lambda \cdot \nabla{f(x_{i})},
	\] где $x_{0}$ будет задаваться некоторым множеством $\text{INIT} = \{x^{0}_{0}, ~ x^{1}_{0}, ~ \ldots, ~ x^{n - 1}_{0}\}$~-- то есть точка, от которой мы собираемся двигаться.

	В качестве промежуточного итога предоставим псевдо-алгоритм для решения этой задачи:
	\begin{lstlisting}
	function $f(x)$:
		/*$implementation~defined$*/
			
	function $\nabla{f(x)}$:
		return $\left[f(x)\dfrac{\partial}{\partial{x^{0}}}, ~ f(x)\dfrac{\partial}{\partial{x^{1}}}, ~ \ldots, ~ f(x)\dfrac{\partial}{\partial{x^{n - 1}}}\right]$
			
	function $gradient(f(x))$:
		$x_{0} \gets \text{INIT}$
		$\lambda \gets \texttt{const}$
		$\forall i \in [1, k]$:
		    $x_{i} \gets x_{i - 1} - \lambda \cdot \nabla{f(x_{i - 1})}$
	\end{lstlisting}
	\subsection*{Пример 1}
	\subsection*{Пример 2}
	\subsection*{Пример 3 с квази-спуском}
	\section*{Задача 2}
	\subsection*{Постановка задачи}
	Реализуйте метод одномерного поиска (метод дихотомии, метод Фибоначчи, метод золотого сечения) и градиентный спуск на его основе.
	\subsection*{Решение}
	Представим себе задачу: мы хотим на непрерывной функции, которая обязательно сходится к некоторому минимуму $M$, найти на некотором интервале её корень. Решая эту задачу простым способом, мы бы могли уйти в долгий, бесконечный или даже бесполезный процесс нахождения приближенного корня заданной функции $f(x)$, ведь на момент поиска мы не знаем: с какой точностью брать значение (то есть, шаг нашего поиска), в какой окрестности лежит корень на интервале (то есть, к чему нам следует сходится, чтобы успешно найти то, что мы ищем).

	Простое решение мы могли бы с легкостью заменить на тот же градиентный спуск с постоянным шагом. Однако, даже с ним мы иногда можем проиграть не только по точности приближенного минимума (если мы ищем минимум за определенное $k$ число шагов), но и вовсе уйти в бесконечный цикл (если мы ищем минимум, пока не будет выполнен критерий $|x_{i} - x_{i - 1}| \leqslant \varepsilon$, где $x_{i} = \{x^0_{i}, ~ x^1_{i}, ~ \ldots, ~ x^{n - 1}_i\}$~-- некоторая координата в $n$-мерном пространстве). По жизни чаще всего случается и другая не менее важная задача: мы хотим найти минимум как можно быстрее и скорее, причем, с точностью до предельно малого $\varepsilon$, в отличие от честного ожидания градиентного спуска.

	Для решения такой столь непосильной задачи мы воспользуемся \textit{методом дихотомии}, или же, как его еще называют, \textit{методом деления отрезка на две части}.

	Но для начала мы посмотрим на работу в одномерном пространстве. В общем случае, этот метод описывается так~-- посмотрим на текущий (может быть начальный, может быть измененный на некотором шаге) интервал $[l, r]$, выберем середину данного отрезка $x$ и сравним со знаком функции в одном из концов: при совпадении, мы перемещаем один конец интервала на точку $x$, в ином случае~-- другой. Отличие начинаются там, где мы решаем подзадачи вида поиска \textit{экстремума функции многих переменных}:
	\[
		\eth = \operatorname*{argmin}_{\eth \in [l, r]}{f(x)}
	\]
	В этом случае на последнем шаге мы смотрим не на знак, а на значение функции $f(x \pm \varepsilon)$ и также здесь смотрим на то, что мы хотим найти: если минимум, то перебрасываем правый конец в рассмотренную точку $x$, в ином случае~-- левый.

	Обозначим за $\varepsilon$~-- как некоторая окрестность минимума, $l, r \in \mathbb{R}$, задана $f(x) : \mathbb{R} \to \mathbb{R}$. Тогда, в качестве промежуточного итога предоставим псевдо-алгоритм для решения этой задачи:
	\begin{lstlisting}
	function $f(x)$:
		/*$implementation~defined$*/

	function $dichotomy2d(l, ~ r, ~ \varepsilon)$:
		$x \gets l$
		$\forall~\infty$:
			$m \gets \dfrac{x + r}{2}$
			$f_{1} \gets f(m + \varepsilon)$
			$f_{2} \gets f(m - \varepsilon)$

			if $|f_{1} - f_{2}| < \varepsilon$ then
				$r \gets m$
			else
				$x \gets m$
	\end{lstlisting}
	Теперь рассмотрим иной, более общий случай, $n$-мерный. Порассуждаем, в чем была проблема градиентного спуска: в описанном выше алгоритме цикл хода по направлению антиградиента (то есть, наискорейшего спуска) ничего не мог сделать в тех случаях, когда до приближенного минимума остается сделать не один гигантский шаг в $learning~rate$ величину, а поменьше, в точности до некоторого малого $\varepsilon$.

	Для исправления столь шальной ситуации, когда алгоритм, который работает не на количество шагов, а~-- на критерий, то есть когда время ожидания отклика программы потребует века, мы будем либо уменьшать шаг в только тех случаях, когда на заданном шаге есть пренеприятный шпион в виде локального минимум на это отрезке, либо не изменять, то есть когда локального минимума нет, а значит нам незачем как-то останавливаться на достигнутом и куда-то сворачивать. Для этого введем специальную функцию $g : [0, 1]$, которая будет возвращать $scale$ нашего шага, причем, так как мы ищем лишь окрестность желаемой точки, то возвращаемое значение будет также учитывать поданный нам $\varepsilon$. Обозначим за $x_{i} = \{x^0_{i}, ~ x^1_{i}, ~ \ldots, ~ x^{n - 1}_i\}$~-- координата точки в $n$-мерном пространстве, $f(x)$~-- заданная функция, $\nabla{f(x)}$~-- градиент функции. Идея~-- изначально мы посчитаем $f(x_{i - 1})$ и $\nabla{f(x_{i - 1})}$, передадим их в функцию для подсчета $scale$, а дальше что есть сил мы будем делать это много раз:
	\begin{enumerate}[(1)]
		\item Положим $m \gets \dfrac{l + r}{2}$ и $\alpha \gets m \pm \varepsilon$.
		\item В качестве $a$ и $b$ разность поданного сверху $f(x_{i - 1})$ и произведения $\nabla{f(x_{i - 1})}$ и $\alpha$.
		\item Рассмотрим два случая: в том случае, если $a < b$, то это значит, что ..., однако в ином же случае~-- .... Таким образом, в первом~-- мы смещаем левую границу, а в втором~-- правую.
	\end{enumerate}
	% TODO --- добавить объяснения, что значит, что a < b или a >= b
	В качестве промежуточного итога предоставим псевдо-алгоритм для решения этой задачи:
	\begin{lstlisting}
	function $f(x)$:
		/*$implementation~defined$*/
	
	function $\nabla{f(x)}$:
		return $\left[f(x)\dfrac{\partial}{\partial{x^{0}}}, ~ f(x)\dfrac{\partial}{\partial{x^{1}}}, ~ \ldots, ~ f(x)\dfrac{\partial}{\partial{x^{n - 1}}}\right]$

	function $scale(p_1, ~ p_2)$:
		$l, r \gets 0, 1$
		$\forall~\infty$:
			$m \gets \dfrac{l + r}{2}$
			$\alpha \gets \lambda \cdot (m \pm \varepsilon)$
			$a, b \gets p_1 - p_2 \cdot \alpha$

			if $a < b$ then
				$l \gets m$
			else
				$r \gets m$

			if $|l - r| \leqslant \varepsilon$ then
				break

	function $gradient\_dichotomy$:
		$x_{0} \gets \text{INIT}$
		$\lambda \gets \texttt{const}$
		$\forall~i \in [1, k]$:
			$x_{i} \gets x_{i - 1} - \lambda \cdot \texttt{scale}(f(x_{i - 1}), \nabla{f(x_{i - 1})})$
	\end{lstlisting}
	\subsection*{Пример 1}
	\subsection*{Пример 2}
	\subsection*{Пример 3}
	\subsection*{Пример 4}
	\section*{Задача 3}
	\subsection*{Постановка задачи}
	Проанализируйте траекторию градиентного спуска на примере квадратичных функций. Для этого придумайте две-три квадратичные функции от двух переменных, на которых работа методов будет отличаться.
	\subsection*{Решение}
	\section*{Задача 4}
	\subsection*{Постановка задачи}
	Для каждой функции:
	\begin{enumerate}[(a)]
		\item исследуйте сходимость градиентного спуска с постоянным шагом, сравните полученные результаты для выбранных функций;
		\item сравните эффективность градиентного спуска с использованием одномерного поиска с точки зрения количества вычислений минимизируемой функции и ее градиентов;
		\item исследуйте работу методов в зависимости от выбора начальной точки;
		\item исследуйте влияние нормализации (scaling) на сходимость на примере масштабирования осей плохо обусловленной функции;
		\item в каждом случае нарисуйте графики с линиями уровня и траекториями методов;
	\end{enumerate}
	\subsection*{Решение}
	\section*{Задача 5}
	\subsection*{Постановка задачи}
	Реализуйте генератор случайных квадратичных функций $n$ переменных с числом обусловленности $k$.
	\subsection*{Решение}
	Для начала поймем, что такое \textit{число обусловленности}. По своей сущности, это нечто, что может показать насколько измениться значение функции при небольшом изменении аргумента. Для нахождения такого числа и, в следствии, нахождения вектора чисел, которые будут являться коэффициентами квадратичной сгенерированной функции. Для решении такой задачи мы могли воспользоваться правильными методами такими как \textit{теорема о сингулярном разложении}: возьмем некоторую матрицу $A$, возьмем его после разложении образовавшийся диагональную матрицу, тогда его (матрицы $A$) число обусловленности будет равно отношению максимального по модулю и минимального по модулю собственных чисел выбранной матрицы. Проблемой столь мощного инструментария заключается в том, что генерация подобной матрицы наивным методом может занимать немереное время, ибо асимптотику спектрального разложения, которое и является основным в сингулярном, никто предугадать не может.

	Тогда приходит идея менее безболезненная, но более радикальная: скажем, что наша матрица изначально была подана диагональной, а значит наша генерации функции сводится к вычислению $n$-чисел на диагонали матрицы.

	Итак, пусть дано число обусловленности $k$, положим $\text{MAX} = k \cdot \text{MIN}$~-- максимальное по модулю значение собственного числа матрицы, а в качестве минимального возьмем случайное число из ограниченного операционной системой диапазоном, например $[0, 2^{\log_{2}{X}} - 1]$. Тогда, наконец, все остальные элементы следует брать из диапазона $[\text{MIN} + 1, \text{MIN} \cdot k)$.

	В качестве промежуточного итога предоставим псевдо-алгоритм для решения этой задачи:
	\begin{lstlisting}
	function $random(l, r)$:
		return $randomized$ $\mathtt{Ret} \in [l, \ldots, r)$
			
	function $generate(n, k)$:
		$\text{MIN} \gets \text{random}(0, 2^{64} - 1)$
		$\text{MAX} \gets \text{MIN} \cdot k$
		$q \gets [\text{MIN},~\text{MAX},~x_{0}~\ldots,~x_{n - 3}], ~ \forall x_{i} = 0$
		$\forall i \in [2, n]$:
			$q_{i} \gets random(\text{MIN} + 1,~\text{MAX})$
	\end{lstlisting}
	\subsection*{Пример c разными $n$, $k$}	
	\section*{Задача 6 и 7}
	\subsection*{Постановка задачи}
	Исследуйте зависимость числа итераций $T(n, k)$, необходимых градиентному спуску для сходимости в зависимости от размерности пространства $2 \leqslant n \leqslant 10^{3}$ и числа обусловленности оптимизируемой функции $1 \leqslant k \leqslant 10^{3}$.
	\subsection*{Решение}
	\subsection*{Исследовательская часть}
	\section*{Дополнительное задание}
	\subsection*{Постановка задачи}
	Реализуйте одномерный поиск с учетом условий Вольфе и исследуйте его эффективность. Сравните полученные результаты с реализованными ранее методами.
	\subsection*{Решение}
	\subsection*{Исследовательская часть}
\end{document}
