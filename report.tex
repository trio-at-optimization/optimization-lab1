\documentclass[12pt, a4paper, oneside]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{soulutf8}
\usepackage{soul}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage[most]{tcolorbox}
\usepackage{listings}
\usepackage[makeroom]{cancel}
\usepackage{tocloft}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{skak}
\usepackage{stmaryrd}
\usepackage{minted}
\usepackage{listings}
\usepackage{color}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{svg}
\setminted[python]{autogobble, breaklines, tabsize = 3}
\setminted[java]{autogobble, breaklines, tabsize = 3}

\hypersetup{
	colorlinks,
	citecolor = black,
	filecolor = black,
	linkcolor = blue,
	urlcolor = blue
}
\lstset{
	basicstyle = \ttfamily,
	mathescape
}

\binoppenalty=10000
\relpenalty=10000
\sloppy

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{\nouppercase{\rightmark\hfill\leftmark}}
\fancyhead[RO]{\nouppercase{\leftmark\hfill\rightmark}}
\fancyfoot[LE,RO]{\hfill\thepage\hfill}

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}

\definecolor{light-gray}{gray}{0.95}
\lstset{columns = fullflexible, basicstyle = \ttfamily, mathescape}
\surroundwithmdframed[hidealllines = true, backgroundcolor = light-gray, innerleftmargin = 15pt, innertopmargin = 0pt, innerbottommargin = 0pt]{lstlisting}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №1}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ Свинцов Михаил
		\end{tabular}
	}
	\vspace*{20em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Задача 1}
	\subsection*{Постановка задачи}
	Реализуйте градиентный спуск с постоянным шагом (learning rate).
	\subsection*{Решение}
	Поймем, сначала, что мы хотим добиться: мы хотели бы найти направление наискорейшего спуска с некоторой точки к минимуму на заданной плоскостью функцией $f$. Однако, при решении этой задачи возникает проблема с производительностью нахождения $\operatorname{argmin}{f}$ за счет появления тех или иных накладных расходов на подсчет не целочисленных значения, а также проблемой с нахождением такого шага $\lambda$, что наш алгоритм не <<застрянет>> в бесконечном поиске интересующей точки.

	Введем обозначения, пусть $\alpha$~-- есть некоторая константа, порядка $10^{-3}$, $x_{i} = \{x^{0}_{i}, x^{1}_{i}, \ldots, x^{n - 1}_{i}\}$~-- некоторая координата в $n$-мерном пространстве, $p_{i}$~-- наше текущее направление.

	Теперь рассмотрим идею \textit{градиентного спуска}: оптимизацию нахождения необходимого минимум за $k$ шагов мы будем осуществлять шаги в $n$-мерном пространстве в направлении, задаваемый как антиградиент функции $f$ в точке, задаваемая предыдущем шагом, то есть
	\[
		x_{i + 1} = x_{i} - \alpha \cdot \nabla{f(x_{i})},
	\] где $x_{0}$ будет задаваться некоторым множеством $\text{INIT} = \{x^{0}_{0}, x^{1}_{0}, \ldots, x^{n - 1}_{0}\}$~-- то есть точка, от которой мы собираемся двигаться.

	Итого, псевдо-алгоритм для этой задачи выглядит следующим образом:
	\begin{lstlisting}
	function $f(x)$:
		/*$implementation~defined$*/
	
	function $\nabla{f(x)}$:
		return $\left[f(x)\dfrac{\partial}{\partial{x^{0}}}, ~ f(x)\dfrac{\partial}{\partial{x^{1}}}, ~ \ldots, ~ f(x)\dfrac{\partial}{\partial{x^{n - 1}}}\right]$
	
	function $main$:
		$x_{0} \gets \text{INIT}$
		$\alpha \gets \texttt{const}$
		forall $i \in [1, k]$ do
			$x_{i} \gets x_{i - 1} - \alpha \cdot \nabla{f(x_{i - 1})}$
	\end{lstlisting}
	Разберем пример. Мы хотим найти методом градиентного спуска приближенный $\operatorname*{argmin}_{\substack{x \in X  \\ y \in Y}}{f(x, y)}$ функции $f(x, y) = x^{2} - (x - y)^{2}$. Его направлением-антиградиентом будет $p_{i} = -\nabla{f(x, y)} = \{(-1) \cdot (2 \cdot x + 2 \cdot (x - y)), (-1) \cdot (-2 \cdot (x - y))\}$. 
	В исходном коде \mintinline{python}|points| представляет из себя шаги, проделываемые алгоритмом от стартового состояния $x_{0}$ до некоторого приближенного $x_{k}$~-- являющийся минимумом. Наконец, представим всему миру полученную картину.
	\begin{flushleft}
		\includesvg[scale = 0.1]{example-gradient.svg}
	\end{flushleft}
\end{document}
