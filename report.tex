\documentclass[12pt, a4paper, oneside]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{soulutf8}
\usepackage{soul}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage[most]{tcolorbox}
\usepackage{listings}
\usepackage[makeroom]{cancel}
\usepackage{tocloft}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{skak}
\usepackage{stmaryrd}
\usepackage{minted}
\usepackage{listings}
\usepackage{color}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{svg}
\setminted[python]{autogobble, breaklines, tabsize = 3}
\setminted[java]{autogobble, breaklines, tabsize = 3}

\hypersetup{
	colorlinks,
	citecolor = black,
	filecolor = black,
	linkcolor = blue,
	urlcolor = blue
}
\lstset{
	basicstyle = \ttfamily,
	mathescape
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{\nouppercase{\rightmark\hfill\leftmark}}
\fancyhead[RO]{\nouppercase{\leftmark\hfill\rightmark}}
\fancyfoot[LE,RO]{\hfill\thepage\hfill}

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}

\definecolor{light-gray}{gray}{0.95}
\lstset{columns = fullflexible, basicstyle = \ttfamily, mathescape}
\surroundwithmdframed[hidealllines = true, backgroundcolor = light-gray, innerleftmargin = 15pt, innertopmargin = 0pt, innerbottommargin = 0pt]{lstlisting}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №1}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ Свинцов Михаил
		\end{tabular}
	}
	\vspace*{20em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Задача 1}
	\subsection*{Постановка задачи}
	Реализуйте градиентный спуск с постоянным шагом (learning rate).
	\subsection*{Решение}
	Поймем, сначала, что мы хотим добиться: мы хотели бы найти направление наискорейшего спуска с некоторой точки к минимуму на заданной плоскостью функцией $f$. Однако, при решении этой задачи возникает проблема с производительностью нахождения $\operatorname{argmin}{f}$ за счет появления тех или иных накладных расходов на подсчет не целочисленных значения, а также проблемой с нахождением такого шага $\lambda$, что наш алгоритм не <<застрянет>> в бесконечном поиске интересующей точки.

	Введем обозначения, пусть $\alpha$~-- есть некоторая константа, порядка $10^{-3}$, $x_{i} = \{x^{0}_{i}, x^{1}_{i}, \ldots, x^{n - 1}_{i}\}$~-- некоторая координата в $n$-мерном пространстве, $p_{i}$~-- наше текущее направление.

	Теперь рассмотрим идею \textit{градиентного спуска}: оптимизацию нахождения необходимого минимум за $k$ шагов мы будем осуществлять шаги в $n$-мерном пространстве в направлении, задаваемый как антиградиент функции $f$ в точке, задаваемая предыдущем шагом, то есть
	\[
		x_{i + 1} = x_{i} - \alpha \cdot \nabla{f(x_{i})},
	\] где $x_{0}$ будет задаваться некоторым множеством $\text{INIT} = \{x^{0}_{0}, x^{1}_{0}, \ldots, x^{n - 1}_{0}\}$~-- то есть точка, от которой мы собираемся двигаться.

	Итого, псевдо-алгоритм для этой задачи выглядит следующим образом:
	\begin{lstlisting}
	function $f(x)$:
		/*$implementation~defined$*/
	
	function $\nabla{f(x)}$:
		return $\left[f(x)\dfrac{\partial}{\partial{x^{0}}}, ~ f(x)\dfrac{\partial}{\partial{x^{1}}}, ~ \ldots, ~ f(x)\dfrac{\partial}{\partial{x^{n - 1}}}\right]$
	
	function $main$:
		$x_{0} \gets \text{INIT}$
		$\alpha \gets \texttt{const}$
		forall $i \in [1, k]$ do
			$x_{i} \gets x_{i - 1} - \alpha \cdot \nabla{f(x_{i - 1})}$
	\end{lstlisting}
	Разберем пример. Мы хотим найти методом градиентного спуска приближенный $\operatorname*{argmin}_{\substack{x \in X  \\ y \in Y}}{f(x, y)}$ функции $f(x, y) = x^{2} - (x - y)^{2}$. Его направлением-антиградиентом будет $p_{i} = -\nabla{f(x, y)} = \{(-1) \cdot (2 \cdot x + 2 \cdot (x - y)), (-1) \cdot (-2 \cdot (x - y))\}$. 
	В исходном коде \mintinline{python}|points| представляет из себя шаги, проделываемые алгоритмом от стартового состояния $x_{0}$ до некоторого приближенного $x_{k}$~-- являющийся минимумом. Наконец, представим всему миру полученную картину.
	\begin{flushleft}
		\includesvg[scale = 0.1]{example-gradient.svg}
	\end{flushleft}
	\section*{Задача 2}
	\subsection*{Постановка задачи}
	Реализуйте метод одномерного поиска (метод дихотомии, метод Фибоначчи, метод золотого сечения) и градиентный спуск на его основе.
	\subsection*{Решение}
	\section*{Задача 3}
	\subsection*{Постановка задачи}
	Проанализируйте траекторию градиентного спуска на примере квадратичных функций. Для этого придумайте две-три квадратичные функции от двух переменных, на которых работа методов будет отличаться.
	\subsection*{Решение}
	\section*{Задача 4}
	\subsection*{Постановка задачи}
	Для каждой функции:
	\begin{enumerate}[(a)]
		\item исследуйте сходимость градиентного спуска с постоянным шагом, сравните полученные результаты для выбранных функций;
		\item сравните эффективность градиентного спуска с использованием одномерного поиска с точки зрения количества вычислений минимизируемой функции и ее градиентов;
		\item исследуйте работу методов в зависимости от выбора начальной точки;
		\item исследуйте влияние нормализации (scaling) на сходимость на примере масштабирования осей плохо обусловленной функции;
		\item в каждом случае нарисуйте графики с линиями уровня и траекториями методов;
	\end{enumerate}
	\subsection*{Решение}
	\section*{Задача 5}
	\subsection*{Постановка задачи}
	Реализуйте генератор случайных квадратичных функций $n$ переменных с числом обусловленности $k$.
	\subsection*{Решение}
	Изначально поймем, что такое \textit{число обусловленности}. По своей сущности, это нечто, что может показать насколько может измениться значение функции при небольшом изменении аргумента. Для нахождения такого числа и, в следствии, нахождения некоторого вектора чисел, которые будут являться коэффициентами квадратичной формы, существует несколько способов: через матричный нормы~-- это исходит напрямую из рассматриваемого линейного уравнения вида $\mathfrak{A}x = \mathbf{b}$, где $\mathfrak{A}$~-- линейный оператор, $\mathbf{b}$~-- вектор и $x$~-- переменная. Такой способ подошел бы нам, если б мы знали заранее нормы матриц. Из-за чего нам понадобится более сильное средство, именуемое как \textit{сингулярное разложение}, а именно: возьмем некоторую матрицу $A$, тогда его число обусловленности будет равно отношению максимального и минимального из диагональных элементов; остальные же элементы на диагональной части матрицы будут коэффициентами разложения квадратичной формы.

	Итак мы хотим сгенерировать матрицу такую, что $k = \dfrac{\max_{\forall i \in [0, n]}{x_{i, i \in [0, n]}}}{\min_{\forall i}{x_{i, i}}}$. Для этого мы получим максимальный элемент $\text{MAX} = k \cdot \text{MIN}$, а в качестве минимального возьмем случайное число из ограниченного операционной системой диапазоном, например $[0, 2^{64} - 1]$. Тогда как все остальные элементы следует брать из диапазона $[\text{MIN} + 1, \text{MIN} \cdot k)$.

	Итого, псевдо-алгоритм для этой задачи выглядит следующим образом:
	\begin{lstlisting}
		function $random(l, r)$:
			return $randomized$ $R \in [l, \ldots, r)$
		
		function $main(n, k)$:
			$\text{MIN} \gets random(0, 2^{32})$
			$\text{MAX} \gets \text{MIN} \cdot k$
			$q \gets [\text{MIN},~\text{MAX},~x_{0}~\ldots,~x_{n - 3}], ~ \forall x_{i} = 0$
			forall $i \in [2, n]$ do
				$q_{i} \gets random(\text{MIN} + 1,~\text{MAX})$
	\end{lstlisting}
	Для примера мы рассмотрим задачу. Необходимо сгенерировать квадратичную функцию с $n = 10$ переменными и $k = 5$ числом обусловленности. Воспользуемся заготовленной программой, которая выводит функцию в \TeX виде, и получим, как один из результатов вот такой:
	\begin{align*}
		f(x_0, x_1, \ldots, x_9) &= 55881652088902977 \cdot x^2_0 + 279408260444514885 \cdot x^2_1 \\
		&+ 168040993445098276 \cdot x^2_2 + 98341599851142922 \cdot x^2_3 \\
		&+ 126881448626083312 \cdot x^2_4 + 112063718763017541 \cdot x^2_5 \\
		&+ 167683450954662177 \cdot x^2_6 + 104734621927684905 \cdot x^2_7 \\
		&+ 196100745993640030 \cdot x^2_8 + 199634427735860479 \cdot x^2_9
	\end{align*}
	\section*{Задача 6}
	\subsection*{Постановка задачи}
	Исследуйте зависимость числа итераций $T(n, k)$, необходимых градиентному спуску для сходимости в зависимости от размерности пространства $2 \leqslant n \leqslant 10^{3}$ и числа обусловленности оптимизируемой функции $1 \leqslant k \leqslant 10^{3}$.
	\subsection*{Решение}
	\section*{Дополнительное задание}
	\subsection*{Постановка задачи}
	Реализуйте одномерный поиск с учетом условий Вольфе и исследуйте его эффективность. Сравните полученные результаты с реализованными ранее методами.
	\subsection*{Решение}
\end{document}
