\documentclass[12pt, a4paper, oneside]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{soulutf8}
\usepackage{soul}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage[most]{tcolorbox}
\usepackage{listings}
\usepackage[makeroom]{cancel}
\usepackage{tocloft}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{skak}
\usepackage{stmaryrd}
\usepackage{listings}
\usepackage{color}
\usepackage[framemethod = tikz]{mdframed}
\usepackage{svg}

\hypersetup{
	colorlinks,
	citecolor = black,
	filecolor = black,
	linkcolor = blue,
	urlcolor = blue
}
\lstset{
	basicstyle = \ttfamily,
	mathescape
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{\nouppercase{\rightmark\hfill\leftmark}}
\fancyhead[RO]{\nouppercase{\leftmark\hfill\rightmark}}
\fancyfoot[LE,RO]{\hfill\thepage\hfill}

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}

\definecolor{light-gray}{gray}{0.95}
\lstset{columns = fullflexible, basicstyle = \ttfamily, mathescape}
\surroundwithmdframed[hidealllines = true, backgroundcolor = light-gray, innerleftmargin = 15pt, innertopmargin = 0pt, innerbottommargin = 0pt]{lstlisting}

\everymath{\displaystyle}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №1}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ Свинцов Михаил
		\end{tabular}
	}
	\vspace*{20em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Задача 1}
	\subsection*{Постановка задачи}
	Реализуйте градиентный спуск с постоянным шагом (learning rate).
	\subsection*{Решение}
	Поймем, изначальное, чего мы хотим добиться: мы хотели бы найти приближенный минимум на плоскости у заданной непрерывной функции $f$. Однако, при наивном решении этой задачи возникает проблема с производительностью нахождения $\operatorname{argmin}{f}$ за счет появления тех или иных накладных расходов на подсчет не целочисленных значения, а также всецело такого алгоритма, который бы с точность до некоторого изменения $\varepsilon$ не <<застрянет>> в бесконечном поиске интересующей точки.

	Для таких целей самым простым, но действующим метод является \textit{градиентный спуск с постоянным шагом}. 	Введем обозначения, пусть $\lambda$~-- есть некоторая константа, порядка $10^{-3}$, $x_{i} = \{x^{0}_{i}, ~ x^{1}_{i}, ~ \ldots, ~ x^{n - 1}_{i}\}$~-- некоторая координата в $n$-мерном пространстве, $p_{i}$~-- наше текущее направление.

	Теперь рассмотрим идею: оптимизацию нахождения необходимого минимум за $k$ шагов мы будем осуществлять шаги в $n$-мерном пространстве в направлении, задаваемый как антиградиент функции $f$ в точке, задаваемая предыдущем шагом, то есть
	\[
		x_{i + 1} = x_{i} - \lambda \cdot \nabla{f(x_{i})},
	\] где $x_{0}$ будет задаваться некоторым множеством $\text{INIT} = \{x^{0}_{0}, ~ x^{1}_{0}, ~ \ldots, ~ x^{n - 1}_{0}\}$~-- то есть точка, от которой мы собираемся двигаться.

	В качестве промежуточного итога предоставим псевдо-алгоритм для решения этой задачи:
	\begin{lstlisting}
	function $f(x)$:
		/*$implementation~defined$*/
			
	function $\nabla{f(x)}$:
		return $\left[f(x)\dfrac{\partial}{\partial{x^{0}}}, ~ f(x)\dfrac{\partial}{\partial{x^{1}}}, ~ \ldots, ~ f(x)\dfrac{\partial}{\partial{x^{n - 1}}}\right]$
			
	function $gradient(f(x))$:
		$x_{0} \gets \text{INIT}$
		$\lambda \gets \texttt{const}$
		$\forall i \in [1, k]$:
		    $x_{i} \gets x_{i - 1} - \lambda \cdot \nabla{f(x_{i - 1})}$
	\end{lstlisting}
	\section*{Задача 2}
	\subsection*{Постановка задачи}
	Реализуйте метод одномерного поиска (метод дихотомии, метод Фибоначчи, метод золотого сечения) и градиентный спуск на его основе.
	\subsection*{Решение}
	\section*{Задача 3}
	\subsection*{Постановка задачи}
	Проанализируйте траекторию градиентного спуска на примере квадратичных функций. Для этого придумайте две-три квадратичные функции от двух переменных, на которых работа методов будет отличаться.
	\subsection*{Решение}
	\section*{Задача 4}
	\subsection*{Постановка задачи}
	Для каждой функции:
	\begin{enumerate}[(a)]
		\item исследуйте сходимость градиентного спуска с постоянным шагом, сравните полученные результаты для выбранных функций;
		\item сравните эффективность градиентного спуска с использованием одномерного поиска с точки зрения количества вычислений минимизируемой функции и ее градиентов;
		\item исследуйте работу методов в зависимости от выбора начальной точки;
		\item исследуйте влияние нормализации (scaling) на сходимость на примере масштабирования осей плохо обусловленной функции;
		\item в каждом случае нарисуйте графики с линиями уровня и траекториями методов;
	\end{enumerate}
	\subsection*{Решение}
	\section*{Задача 5}
	\subsection*{Постановка задачи}
	Реализуйте генератор случайных квадратичных функций $n$ переменных с числом обусловленности $k$.
	\subsection*{Решение}
	Для начала поймем, что такое \textit{число обусловленности}. По своей сущности, это нечто, что может показать насколько измениться значение функции при небольшом изменении аргумента. Для нахождения такого числа и, в следствии, нахождения вектора чисел, которые будут являться коэффициентами квадратичной сгенерированной функции. Для решении такой задачи мы могли воспользоваться правильными методами такими как \textit{теорема о сингулярном разложении}: возьмем некоторую матрицу $A$, возьмем его после разложении образовавшийся диагональную матрицу, тогда его (матрицы $A$) число обусловленности будет равно отношению максимального по модулю и минимального по модулю собственных чисел выбранной матрицы. Проблемой столь мощного инструментария заключается в том, что генерация подобной матрицы наивным методом может занимать немереное время, ибо асимптотику спектрального разложения, которое и является основным в сингулярном, никто предугадать не может.

	Тогда приходит идея менее безболезненная, но более радикальная: скажем, что наша матрица изначально была подана диагональной, а значит наша генерации функции сводится к вычислению $n$-чисел на диагонали матрицы.

	Итак, пусть дано число обусловленности $k$, положим $\text{MAX} = k \cdot \text{MIN}$~-- максимальное по модулю значение собственного числа матрицы, а в качестве минимального возьмем случайное число из ограниченного операционной системой диапазоном, например $[0, 2^{\log_{2}{X}} - 1]$. Тогда, наконец, все остальные элементы следует брать из диапазона $[\text{MIN} + 1, \text{MIN} \cdot k)$.

	В качестве промежуточного итога предоставим псевдо-алгоритм для решения этой задачи:
	\begin{lstlisting}
	function $random(l, r)$:
		return $randomized$ $\mathtt{Ret} \in [l, \ldots, r)$
			
	function $generate(n, k)$:
		$\text{MIN} \gets \text{random}(0, 2^{64} - 1)$
		$\text{MAX} \gets \text{MIN} \cdot k$
		$q \gets [\text{MIN},~\text{MAX},~x_{0}~\ldots,~x_{n - 3}], ~ \forall x_{i} = 0$
		$\forall i \in [2, n]$:
			$q_{i} \gets random(\text{MIN} + 1,~\text{MAX})$
	\end{lstlisting}
	\section*{Задача 6 и 7}
	\subsection*{Постановка задачи}
	Исследуйте зависимость числа итераций $T(n, k)$, необходимых градиентному спуску для сходимости в зависимости от размерности пространства $2 \leqslant n \leqslant 10^{3}$ и числа обусловленности оптимизируемой функции $1 \leqslant k \leqslant 10^{3}$.
	\subsection*{Решение}
	\section*{Дополнительное задание}
	\subsection*{Постановка задачи}
	Реализуйте одномерный поиск с учетом условий Вольфе и исследуйте его эффективность. Сравните полученные результаты с реализованными ранее методами.
	\subsection*{Решение}
%	\section*{Задача 1}
%	\subsection*{Постановка задачи}
%	Реализуйте градиентный спуск с постоянным шагом (learning rate).
%	\subsection*{Решение}
%	Поймем, сначала, что мы хотим добиться: мы хотели бы найти направление наискорейшего спуска с некоторой точки к минимуму на заданной плоскостью функцией $f$. Однако, при решении этой задачи возникает проблема с производительностью нахождения $\operatorname{argmin}{f}$ за счет появления тех или иных накладных расходов на подсчет не целочисленных значения, а также проблемой с нахождением такого шага $\lambda$, что наш алгоритм не <<застрянет>> в бесконечном поиске интересующей точки.
%
%	Введем обозначения, пусть $\alpha$~-- есть некоторая константа, порядка $10^{-3}$, $x_{i} = \{x^{0}_{i}, x^{1}_{i}, \ldots, x^{n - 1}_{i}\}$~-- некоторая координата в $n$-мерном пространстве, $p_{i}$~-- наше текущее направление.
%
%	Теперь рассмотрим идею \textit{градиентного спуска}: оптимизацию нахождения необходимого минимум за $k$ шагов мы будем осуществлять шаги в $n$-мерном пространстве в направлении, задаваемый как антиградиент функции $f$ в точке, задаваемая предыдущем шагом, то есть
%	\[
%		x_{i + 1} = x_{i} - \alpha \cdot \nabla{f(x_{i})},
%	\] где $x_{0}$ будет задаваться некоторым множеством $\text{INIT} = \{x^{0}_{0}, x^{1}_{0}, \ldots, x^{n - 1}_{0}\}$~-- то есть точка, от которой мы собираемся двигаться.
%
%	Итого, псевдо-алгоритм для этой задачи выглядит следующим образом:
%	\begin{lstlisting}
%	function $f(x)$:
%		/*$implementation~defined$*/
%	
%	function $\nabla{f(x)}$:
%		return $\left[f(x)\dfrac{\partial}{\partial{x^{0}}}, ~ f(x)\dfrac{\partial}{\partial{x^{1}}}, ~ \ldots, ~ f(x)\dfrac{\partial}{\partial{x^{n - 1}}}\right]$
%	
%	function $main$:
%		$x_{0} \gets \text{INIT}$
%		$\alpha \gets \texttt{const}$
%		forall $i \in [1, k]$ do
%			$x_{i} \gets x_{i - 1} - \alpha \cdot \nabla{f(x_{i - 1})}$
%	\end{lstlisting}
%	Разберем пример. Мы хотим найти методом градиентного спуска приближенный $\operatorname*{argmin}_{\substack{x \in X  \\ y \in Y}}{f(x, y)}$ функции $f(x, y) = x^{2} - (x - y)^{2}$. Его направлением-антиградиентом будет $p_{i} = -\nabla{f(x, y)} = \{(-1) \cdot (2 \cdot x + 2 \cdot (x - y)), (-1) \cdot (-2 \cdot (x - y))\}$. 
%	В исходном коде \texttt{points} представляет из себя шаги, проделываемые алгоритмом от стартового состояния $x_{0}$ до некоторого приближенного $x_{k}$~-- являющийся минимумом. Наконец, представим всему миру полученную картину.
%	\begin{flushleft}
%		\includesvg[scale = 0.1]{example-gradient.svg}
%	\end{flushleft}
%	\section*{Задача 2}
%	\subsection*{Постановка задачи}
%	Реализуйте метод одномерного поиска (метод дихотомии, метод Фибоначчи, метод золотого сечения) и градиентный спуск на его основе.
%	\subsection*{Решение}
%	Представим себе задачу: мы хотим на непрерывной функции, которая обязательно сходится к некоторому минимуму $M$, найти на некотором интервале её корень. Решая эту задачу простым способом, мы бы могли уйти в долгий, бесконечный или даже бесполезный процесс нахождения приближенного корня заданной функции $f(x)$, ведь на момент поиска мы не знаем: с какой точностью брать значение (то есть, шаг нашего поиска), в какой окрестности лежит корень на интервале (то есть, к чему нам следует сходится, чтобы успешно найти то, что мы ищем). Для этого мы применим метод \textit{дихотомии}, или же \textit{методом деления отрезка на две части}.
%
%	В общем случае, этот метод описывается так~-- посмотрим на текущий (может быть начальный, может быть измененный на некотором шаге) интервал $[l, r]$, выберем середину данного отрезка $x$ и сравним со знаком функции в одном из концов: при совпадении, мы перемещаем один конец интервала на точку $x$, в ином случае~-- другой. Отличие начинаются там, где мы решаем подзадачи вида поиска \textit{экстремума функции многих переменных}:
%	\[
%		\eth = \operatorname*{argmin}_{\eth \in [l, r]}{f(x)}
%	\]
%	В этом случае на последнем шаге мы смотрим не на знак, а на значение функции $f(x \pm \varepsilon)$ и также здесь смотрим на то, что мы хотим найти: если минимум, то перебрасываем правый конец в рассмотренную точку $x$, в ином случае~-- левый.
%
%	Пусть $\varepsilon$~-- некоторая окрестность минимума/максимума; $l, r \in \mathbb{R}$; задана $f(x) : \mathbb{R} \to \mathbb{R}$. Тогда, псевдо-алгоритм для задачи реализации метода одномерного поиска выглядит следующим образом:
%	\begin{lstlisting}
%		function $f(x)$:
%			/*$implementation~defined$*/
%		
%		function $find(l, ~ r, ~ \varepsilon, ~ \texttt{isMin})$:
%			$x \gets l$
%			forall $\infty$ do
%				$m \gets \dfrac{x + r}{2}$
%				$f_{1} \gets f(m + \varepsilon)$
%				$f_{2} \gets f(m - \varepsilon)$
%
%				if isMin then
%					if $|f_{1} - f_{2}| < \varepsilon$ then
%						$r \gets m$
%					else
%						$x \gets m$
%				then
%					if $|f_{1} - f_{2}| < \varepsilon$ then
%						$x \gets m$
%					else
%						$r \gets m$
%
%				if $|x - r| \leqslant \varepsilon$ then
%					break
%			return $x$
%	\end{lstlisting}
%	Рассмотрим пример одномерного поиска. Возьмем функцию $f(x) = x^{2} - e^{x}$ и промежуток $[0, 2]$. Также, к качестве окрестности точки $x'$ (найденная) $\varepsilon = 0.001$. Тогда, по нашему алгоритму выходит вот такой график:
%	\begin{flushleft}
%		\includesvg[scale = 1]{example-dichotomy.svg}
%	\end{flushleft}
%	Здесь синим цветом указана функция $f(x)$ на промежутке $[0, 2]$, а красным~-- найденный экстремум (в данном случае, минимум) функции, которая приблизительно равна $x' \approx 1.9990234375$.
%
%	Теперь рассмотрим задачу немного другого вида. Скажем, что у нас функция $find$ теперь ищет только минимум на заданном промежутке. Тогда, рассмотрим координату $x_{i} = \{x^{0}_{i}, x^{1}_{i}, \ldots, x^{n - 1}_{i}\}$ в $n$-мерном пространстве, удовлетворяющее свойству:
%	\[
%		x_{i} \in \operatorname*{argmin}_{\substack{x \in X \\ y \in Y}}{f(x_{i})}
%	\]
%	Задача: реализовать градиентный спуск на основе метода дихотомии. Как уже было сказано ранее, данный метод ищет на отрезке непрерывной функции, которая к чему-то на данном интервале сходится. Рассмотрим интервал $[x_{0}, x_{i}]$~-- это путь, по которому мы передвигаемся по направлению антиградиента функции $f(x_{i})$ с некоторым постоянным шагом $\alpha$. В чем проблема такой незамысловатой инженерной мысли? В том, что мы можем двигаться чрезвычайно медленно и не очень правильно: дело в том, что в алгоритме мы указываем число шагов, за которые мы бы очень хотели дойти до минимума, что, справедливо, не факт, что мы дойдем до него (хотя направление вполне можем иметь верное). Для исправления подключается метод дихотомии и говорит: давайте на каждом шаге мы будем выбирать такую точку $x_{i}$, где значение функции меньше всего из рассматриваемого отрезка (которое мы изначально зададим как $[l,~r] : \exists [l',~r']$ на котором лежит минимум) и уже зная аргумент, куда потенциально приведет нас эта точка, пойдем туда по направлению анти-градиента. Таким образом, мы значительно сокращаем количество итераций, как раз за счет того, что мы на каждом шаге пытаемся найти минимум.
%
%	Итак, пусть $x_{i} = \{x^{0}_{i}, ~ x^{1}_{i}, ~ \ldots, x^{n - 1}_{i}\}$~-- $i$-ая координата в $n$-мерном пространстве; задана функция $f(x_{i}) : \mathbb{R}^{n} \to \mathbb{R}$; координаты $\texttt{INIT} = \{x^{0}_{0}, ~ x^{1}_{0}, ~ \ldots, ~ x^{n - 1}_{0}\}$~--- начальная точка, от которой нам следует двигаться; число шагов $k$, шаг $\alpha$ и окрестность $\varepsilon$. Тогда, псевдо-алгоритм для этой задачи выглядит следующим образом:
%	\begin{lstlisting}
%		function $f(x)$:
%			/*$implementation~defined$*/
%		
%		function $\nabla{f(x)}$:
%			return $\left[f(x)\dfrac{\partial}{\partial{x^{0}}}, ~ f(x)\dfrac{\partial}{\partial{x^{1}}}, ~ \ldots, ~ f(x)\dfrac{\partial}{\partial{x^{n - 1}}}\right]$
%
%		function $find(a,~\varepsilon)$:
%			$x \gets a$
%			forall $\infty$ do
%				$m \gets \dfrac{x + r}{2}$
%				$f_{1} \gets f(m + \varepsilon)$
%				$f_{2} \gets f(m - \varepsilon)$
%
%				if $|f_{1} - f_{2}| < \varepsilon$ then
%					$x \gets m$
%				else
%					$r \gets m$
%
%				if $|x - r| \leqslant \varepsilon$ then
%					break
%			return $x$
%
%		function $main$:
%			$x_{0} \gets~$INIT
%			$\alpha \gets~$const
%			$\varepsilon \gets~$const
%			forall $i \in [1, k]$ do
%				$x_{i} \gets x_{0} - \alpha \cdot \nabla{f(x_{0})}$
%			$x_{0} \gets \text{find}(x_{0},~r,~\varepsilon)$
%	\end{lstlisting}
%	Рассмотрим пример использования алгоритма. Пусть $f(x, y)$~-- это функция Розенброка, или $(1 - x)^{2} + 100 \cdot (y - x^{2})^{2}$. Данная функция является отличным примером для проверки правильности алгоритма, засчет того, что минимум $f(x, y) = 0$ в точке $(1,~1)$.
%	\begin{flushleft}
%		\includesvg[scale = 1]{example-dichotomy-with-gradient.svg}
%	\end{flushleft}
%	\section*{Задача 3}
%	\subsection*{Постановка задачи}
%	Проанализируйте траекторию градиентного спуска на примере квадратичных функций. Для этого придумайте две-три квадратичные функции от двух переменных, на которых работа методов будет отличаться.
%	\subsection*{Решение}
%	\section*{Задача 4}
%	\subsection*{Постановка задачи}
%	Для каждой функции:
%	\begin{enumerate}[(a)]
%		\item исследуйте сходимость градиентного спуска с постоянным шагом, сравните полученные результаты для выбранных функций;
%		\item сравните эффективность градиентного спуска с использованием одномерного поиска с точки зрения количества вычислений минимизируемой функции и ее градиентов;
%		\item исследуйте работу методов в зависимости от выбора начальной точки;
%		\item исследуйте влияние нормализации (scaling) на сходимость на примере масштабирования осей плохо обусловленной функции;
%		\item в каждом случае нарисуйте графики с линиями уровня и траекториями методов;
%	\end{enumerate}
%	\subsection*{Решение}
%	\section*{Задача 5}
%	\subsection*{Постановка задачи}
%	Реализуйте генератор случайных квадратичных функций $n$ переменных с числом обусловленности $k$.
%	\subsection*{Решение}
%	Изначально поймем, что такое \textit{число обусловленности}. По своей сущности, это нечто, что может показать насколько может измениться значение функции при небольшом изменении аргумента. Для нахождения такого числа и, в следствии, нахождения некоторого вектора чисел, которые будут являться коэффициентами квадратичной формы, существует несколько способов: через матричный нормы~-- это исходит напрямую из рассматриваемого линейного уравнения вида $\mathfrak{A}x = \mathbf{b}$, где $\mathfrak{A}$~-- линейный оператор, $\mathbf{b}$~-- вектор и $x$~-- переменная. Такой способ подошел бы нам, если б мы знали заранее нормы матриц. Из-за чего нам понадобится более сильное средство, именуемое как \textit{сингулярное разложение}, а именно: возьмем некоторую матрицу $A$, тогда его число обусловленности будет равно отношению максимального и минимального из диагональных элементов; остальные же элементы на диагональной части матрицы будут коэффициентами разложения квадратичной формы.
%
%	Итак мы хотим сгенерировать матрицу такую, что $k = \dfrac{\max_{\forall i \in [0, n]}{x_{i, i \in [0, n]}}}{\min_{\forall i}{x_{i, i}}}$. Для этого мы получим максимальный элемент $\text{MAX} = k \cdot \text{MIN}$, а в качестве минимального возьмем случайное число из ограниченного операционной системой диапазоном, например $[0, 2^{64} - 1]$. Тогда как все остальные элементы следует брать из диапазона $[\text{MIN} + 1, \text{MIN} \cdot k)$.
%
%	Итого, псевдо-алгоритм для этой задачи выглядит следующим образом:
%	\begin{lstlisting}
%		function $random(l, r)$:
%			return $randomized$ $R \in [l, \ldots, r)$
%		
%		function $main(n, k)$:
%			$\text{MIN} \gets \text{random}(0, 2^{64} - 1)$
%			$\text{MAX} \gets \text{MIN} \cdot k$
%			$q \gets [\text{MIN},~\text{MAX},~x_{0}~\ldots,~x_{n - 3}], ~ \forall x_{i} = 0$
%			forall $i \in [2, n]$ do
%				$q_{i} \gets random(\text{MIN} + 1,~\text{MAX})$
%	\end{lstlisting}
%	Для примера мы рассмотрим задачу. Необходимо сгенерировать квадратичную функцию с $n = 10$ переменными и $k = 5$ числом обусловленности. Воспользуемся заготовленной программой, которая выводит функцию в \TeX виде, и получим, как один из результатов вот такой:
%	\begin{align*}
%		f(x_0, x_1, \ldots, x_9) &= 55881652088902977 \cdot x^2_0 + 279408260444514885 \cdot x^2_1 \\
%		&+ 168040993445098276 \cdot x^2_2 + 98341599851142922 \cdot x^2_3 \\
%		&+ 126881448626083312 \cdot x^2_4 + 112063718763017541 \cdot x^2_5 \\
%		&+ 167683450954662177 \cdot x^2_6 + 104734621927684905 \cdot x^2_7 \\
%		&+ 196100745993640030 \cdot x^2_8 + 199634427735860479 \cdot x^2_9
%	\end{align*}
%	\section*{Задача 6 и 7}
%	\subsection*{Постановка задачи}
%	Исследуйте зависимость числа итераций $T(n, k)$, необходимых градиентному спуску для сходимости в зависимости от размерности пространства $2 \leqslant n \leqslant 10^{3}$ и числа обусловленности оптимизируемой функции $1 \leqslant k \leqslant 10^{3}$.
%	\subsection*{Решение}
%	Для решения этой задачи нам понадобится решение \textit{первой задачи}, а именно метод, используемый там. Кратко напомним, что такое метод градиентного спуска. Пусть у нас задана точка $x_{i} = \{x^0_i, ~ x^1_i, ~ \ldots, ~ x^{n - 1}_i\}$~-- координата в $n$-мерном пространстве~-- тогда, чтобы найти приближенно какой-то минимум (ведь, их бывает по жизни несколько) двигаясь по направлению, противоположный наибыстрейшему подъему, с неким $\alpha$~-- константой, значащий шаг в нашем \texttt{for}-е, и какое-то предельное число шагов $k$. Формулой для всего этого, как мы знаем, была
%	\[
%		x_{i} = x_{i - 1} - \alpha \cdot \nabla{f(x_{i - 1})},
%	\] где $x_{0} = \text{INIT} = \{x^0_0, ~ x^1_0, ~ \ldots, ~ x^{n - 1}_0\}$~-- начальная точка, от которой нам бы хотелось найти минимум.
%
%	Для исследования, мы зададим для $\forall \langle n, k \rangle$ некоторое число шагов, которое является предельным. Кроме того, мы можем воспользоваться критерием остановки ??? для понимания, есть ли нам еще смысл идти дальше, или мы можем остаться с нынешними значениями (наше значение измениться не более, чем на $\varepsilon$).
%
%	Итак, пусть $\hat{k}$~-- максимальное количество шагов, допустимые; $n$, $k$~-- входные данные; $x_{i} = \{x^0_i, ~ x^1_i, ~ \ldots, ~ x^{n - 1}_{i}\}$~-- $i$-ая координата в $n$-мерном пространстве; $\text{INIT} = \{x^0_0, ~ x^1_0, ~ \ldots, ~ x^{n - 1}_0\}$~-- начальная координата, с которой нам следует двигаться; $f(x)$~-- сгенерированная функция с $n$ переменными и числом $k$ обусловленности; $\nabla{f(x)}$~-- градиент сгенерированной функции; $g(x)$~-- функция проверки критерия остановки; $\alpha$~-- шаг. Тогда, псевдо-алгоритм для этой задачи выглядит следующим образом:
%	\begin{lstlisting}
%		function $f(x)$:
%			/*$implementation~defined$*/
%
%		function $\nabla{f(x)}$:
%			return $\left[f(x)\dfrac{\partial}{\partial{x^0}}, ~ f(x)\dfrac{\partial}{\partial{x^1}}, ~ \ldots, ~ f(x)\dfrac{\partial}{\partial{x^{n - 1}}}\right]$
%
%		function $random(l, r)$:
%			return $randomized$ $R \in [l, \ldots, r)$
%		
%		function $get(n, k)$:
%			$\text{MIN} \gets \text{random}(0, 2^{64} - 1)$
%			$\text{MAX} \gets \text{MIN} \cdot k$
%			$q \gets [\text{MIN},~\text{MAX},~x_{0}~\ldots,~x_{n - 3}], ~ \forall x_{i} = 0$
%			forall $i \in [2, n - 1]$ do
%				$q_{i} \gets random(\text{MIN} + 1,~\text{MAX})$
%			return $Q : L \to K \iff \sum_{i = 0}^{n - 1}{q_{i} \cdot x^{i}_{i, ~ i}}$
%
%		function $main$:
%			forall $\langle n', ~ k' \rangle \in \{\langle n, ~ k \rangle\}$:
%				$x_{0} \gets \text{INIT}$
%				$\alpha \gets \texttt{const}$
%				forall $i \in [1, \hat{k}]$ do
%					if $g(x_{i - 1})$ then
%						break
%					$x_{i} \gets x_{i - 1} - \alpha \cdot \nabla{f(x_{i - 1})}$
%	\end{lstlisting}
%	РЕЗУЛЬТАТЫ ТЕСТИРОВАНИЯ.
%	\section*{Дополнительное задание}
%	\subsection*{Постановка задачи}
%	Реализуйте одномерный поиск с учетом условий Вольфе и исследуйте его эффективность. Сравните полученные результаты с реализованными ранее методами.
%	\subsection*{Решение}
\end{document}
